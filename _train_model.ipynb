{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing methods of hurricane forecast uncertainty\n",
    "##### author: Elizabeth A. Barnes, Randal J. Barnes and Mark DeMaria\n",
    "##### version: v0.1.0\n",
    "\n",
    "```\n",
    "conda create --name env-hurr-tfp python=3.9\n",
    "conda activate env-hurr-tfp\n",
    "pip install tensorflow==2.7.0\n",
    "pip install tensorflow-probability==0.15.0\n",
    "pip install --upgrade numpy scipy pandas statsmodels matplotlib seaborn \n",
    "pip install --upgrade palettable progressbar2 tabulate icecream flake8\n",
    "pip install --upgrade keras-tuner sklearn\n",
    "pip install --upgrade jupyterlab black isort jupyterlab_code_formatter\n",
    "pip install silence-tensorflow\n",
    "pip install tqdm\n",
    "```\n",
    "\n",
    "Use the command\n",
    "```python -m pip freeze > requirements.txt```\n",
    "to make a pip installation list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "import experiment_settings\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from build_data import build_hurricane_data\n",
    "from build_model import build_shash_model, build_bnn_model\n",
    "from custom_loss import compute_shash_NLL, compute_NLL\n",
    "from custom_metrics import CustomMAE, InterquartileCapture, SignTest\n",
    "from model_diagnostics import plot_history\n",
    "from save_model_run import save_model_run\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras import optimizers\n",
    "from training_instrumentation import TrainingInstrumentation\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "silence_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Randal J Barnes and Elizabeth A. Barnes\"\n",
    "__version__ = \"18 December 2021\"\n",
    "\n",
    "# EXP_NAME = \"intensity1_AL72\"\n",
    "# EXP_NAME = \"intensity5_EPCP72\"\n",
    "# EXP_NAME = \"intensity102_EPCP48\"\n",
    "# EXP_NAME = \"intensity107_EPCP48\"\n",
    "\n",
    "DATA_PATH = \"data/\"\n",
    "MODEL_PATH = \"saved_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"figure.dpi\"] = 150\n",
    "np.warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "settings = experiment_settings.get_settings(EXP_NAME)\n",
    "pprint.pprint(settings, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the intensity data tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    x_train,\n",
    "    onehot_train,\n",
    "    x_val,\n",
    "    onehot_val,\n",
    "    data_summary,\n",
    "    df,\n",
    ") = build_hurricane_data(DATA_PATH, settings, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystoping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=settings[\"patience\"],\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "training_callback = TrainingInstrumentation(\n",
    "    x_train,\n",
    "    onehot_train,\n",
    "    interval=10,\n",
    ")\n",
    "\n",
    "callbacks = [earlystoping_callback, training_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_SEED_LIST = [settings[\"rng_seed\"]]\n",
    "\n",
    "# NETWORK_SEED_LIST = [\n",
    "#     18311, 59811, 96605, 57122, 71786, 67878, 33152, 22416, 81168, 27531,\n",
    "#     52157,   455, 98106, 37162, 58090, 90264, 25338, 27555, 92859, 13387,\n",
    "#     74723, 19736, 36842, 68050, 59711, 95199, 66418, 68997, 53431, 37786,\n",
    "#     79742, 74042,  8347, 49338, 96884, 14870, 88326,   921, 79436, 23564,\n",
    "#     2171,  89287, 31264, 22974, 31029, 97532,  4118, 20170, 77804, 67085,\n",
    "#     24752, 29814, 62255, 23602,  9709, 76607, 49259, 32678, 56290, 53251,\n",
    "#     66300, 40562, 10800, 94890, 14329, 94699, 43680, 71747, 64795, 68908,\n",
    "#     50914, 43933, 63123, 62693, 48125,  1701, 15552, 25529, 54440,  4657,\n",
    "#     9542,  77033, 96934, 49582, 34698, 82159, 20147, 83905, 17808, 21539,\n",
    "#     59896, 70793, 97321, 92170, 24675, 82172, 24226, 30460, 53389, 61562,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for network_seed in NETWORK_SEED_LIST:\n",
    "    tf.random.set_seed(network_seed)  # This sets the global random seed.\n",
    "\n",
    "    # Create the model name.\n",
    "    model_name = (\n",
    "        EXP_NAME + \"_\" + settings[\"uncertainty_type\"] + '_' + f\"network_seed_{network_seed}_rng_seed_{settings['rng_seed']}\"\n",
    "    )\n",
    "    pprint.pprint(model_name)\n",
    "\n",
    "    # Make, compile, and train the model\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    if settings[\"uncertainty_type\"] == \"bnn\":       \n",
    "        model = build_bnn_model(\n",
    "            x_train,\n",
    "            onehot_train,\n",
    "            hiddens=settings[\"hiddens\"],\n",
    "            output_shape=onehot_train.shape[1],\n",
    "            ridge_penalty=settings[\"ridge_param\"],\n",
    "            act_fun=settings[\"act_fun\"],\n",
    "        )        \n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(\n",
    "                learning_rate=settings[\"learning_rate\"],\n",
    "            ),\n",
    "            loss=compute_NLL,\n",
    "        )        \n",
    "        \n",
    "    elif settings[\"uncertainty_type\"][:5] == \"shash\":   \n",
    "        model = build_shash_model(\n",
    "            x_train,\n",
    "            onehot_train,\n",
    "            hiddens=settings[\"hiddens\"],\n",
    "            output_shape=onehot_train.shape[1],\n",
    "            ridge_penalty=settings[\"ridge_param\"],\n",
    "            act_fun=settings[\"act_fun\"],\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizers.SGD(\n",
    "                learning_rate=settings[\"learning_rate\"],\n",
    "                momentum=settings[\"momentum\"],\n",
    "                nesterov=settings[\"nesterov\"],\n",
    "            ),\n",
    "            loss=compute_shash_NLL,\n",
    "            metrics=[\n",
    "                CustomMAE(name=\"custom_mae\"),\n",
    "                InterquartileCapture(name=\"interquartile_capture\"),\n",
    "                SignTest(name=\"sign_test\"),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        \n",
    "    model.summary()\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        onehot_train,\n",
    "        validation_data=(x_val, onehot_val),\n",
    "        batch_size=settings[\"batch_size\"],\n",
    "        epochs=settings[\"n_epochs\"],\n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    stop_time = time.time()\n",
    "\n",
    "    # Display the results, and save the model rum.\n",
    "    best_epoch = np.argmin(history.history[\"val_loss\"])\n",
    "    fit_summary = {\n",
    "        \"network_seed\": network_seed,\n",
    "        \"elased_time\": stop_time - start_time,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"loss_train\": history.history[\"loss\"][best_epoch],\n",
    "        \"loss_valid\": history.history[\"val_loss\"][best_epoch],\n",
    "    }\n",
    "    pprint.pprint(fit_summary, width=80)\n",
    "    plot_history(history, model_name)\n",
    "\n",
    "    save_model_run(\n",
    "        data_summary,\n",
    "        fit_summary,\n",
    "        model,\n",
    "        MODEL_PATH,\n",
    "        model_name,\n",
    "        settings,\n",
    "        __version__,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + EXP_NAME + \" training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
