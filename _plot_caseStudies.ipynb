{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and visualize case studies\n",
    "##### author: Elizabeth A. Barnes, Randal J. Barnes and Mark DeMaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Python setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "import experiment_settings\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import shash\n",
    "from build_data import build_hurricane_data\n",
    "import build_model\n",
    "import model_diagnostics\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "import prediction\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import analysis_plots\n",
    "from scipy import stats\n",
    "from toolbox import custom_round, ceiling_round\n",
    "import seaborn as sns\n",
    "\n",
    "import imp\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "silence_tensorflow()\n",
    "dpiFig = 400\n",
    "\n",
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"figure.dpi\"] = 150\n",
    "np.warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## User parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_PATH = \"saved_predictions/\"\n",
    "df_bestval = pd.read_pickle(PREDICTION_PATH + \"best_shash3_validation_seeds.pickle\")\n",
    "df_bestval\n",
    "df_bestval[df_bestval[\"basin_lead\"]==\"AL72\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__  = \"Randal J Barnes and Elizabeth A. Barnes\"\n",
    "__version__ = \"19 March 2022\"\n",
    "\n",
    "EXP_NAME_LIST = (\"intensity203_AL72\",    \n",
    "                )\n",
    "\n",
    "PLOT_SEED_VECTOR = {\"intensity301_EPCP24\": 416,\n",
    "                    \"intensity302_EPCP48\": 416,\n",
    "                    \"intensity303_EPCP72\": 222,                    \n",
    "                    \"intensity202_AL48\": 416,\n",
    "                    # \"intensity201_AL24\": 333,\n",
    "                    \"intensity201_AL24\": 739,                    \n",
    "                    \"intensity203_AL72\": 739,#416,#739,\n",
    "                    \"intensity204_AL96\": 333,                    \n",
    "                   }\n",
    "TESTING_YEAR_VECTOR = {\"intensity301_EPCP24\": 2020,\n",
    "                       \"intensity302_EPCP48\": 2020,\n",
    "                       \"intensity303_EPCP72\": 2020,\n",
    "                       \"intensity202_AL48\": 2018,\n",
    "                       # \"intensity201_AL24\": 2018,\n",
    "                       \"intensity201_AL24\": 2017,                       \n",
    "                       \"intensity203_AL72\": 2018,#2017,#2018,          \n",
    "                       \"intensity204_AL96\": 2018,                                 \n",
    "                      }\n",
    "                        \n",
    "    \n",
    "\n",
    "SHASH_INCS = np.arange(-160,161,1)\n",
    "DATA_PATH = \"data/\"\n",
    "MODEL_PATH = \"saved_models/\"\n",
    "FIGURE_PATH = \"figures/paper_figures/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RI_THRESH_DICT = {24: 30,\n",
    "                  48: 55,\n",
    "                  72: 65,\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FS = 16\n",
    "colors = ('#284E60','#E1A730','#D95980','#C3B1E1','#351F27','#A9C961')\n",
    "\n",
    "### for white background...\n",
    "plt.rc('text',usetex=True)\n",
    "plt.rc('font',**{'family':'sans-serif','sans-serif':['Avant Garde']}) \n",
    "plt.rc('savefig',facecolor='white')\n",
    "plt.rc('axes',facecolor='white')\n",
    "plt.rc('axes',labelcolor='dimgrey')\n",
    "plt.rc('axes',labelcolor='dimgrey')\n",
    "plt.rc('xtick',color='dimgrey')\n",
    "plt.rc('ytick',color='dimgrey')\n",
    "################################  \n",
    "################################  \n",
    "def adjust_spines(ax, spines):\n",
    "    for loc, spine in ax.spines.items():\n",
    "        if loc in spines:\n",
    "            spine.set_position(('outward', 5))\n",
    "        else:\n",
    "            spine.set_color('none')  \n",
    "    if 'left' in spines:\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "    else:\n",
    "        ax.yaxis.set_ticks([])\n",
    "    if 'bottom' in spines:\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "    else:\n",
    "        ax.xaxis.set_ticks([]) \n",
    "\n",
    "def format_spines(ax):\n",
    "    adjust_spines(ax, ['left', 'bottom'])\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['left'].set_color('dimgrey')\n",
    "    ax.spines['bottom'].set_color('dimgrey')\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "    ax.tick_params('both',length=4,width=2,which='major',color='dimgrey')\n",
    "#     ax.yaxis.grid(zorder=1,color='dimgrey',alpha=0.35)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_storm_details(df, isample):\n",
    "    storm = df.iloc[isample]\n",
    "    storm_name = storm['Name']\n",
    "    storm_ftime = storm['ftime(hr)']\n",
    "    storm_month = str(storm['time'])[:-4]\n",
    "    storm_day = str(storm['time'])[-4:-2]\n",
    "    storm_hour = str(storm['time'])[-2:]\n",
    "    storm_year = storm['year']\n",
    "\n",
    "    details = storm_name + ' ' + str(storm_year) + '-' + str(storm_month) + '-' + str(storm_day) + ' ' + str(storm_hour) + '00 @' + str(storm_ftime) + 'hr'\n",
    "    \n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_iqr_vs_error(ax,model, x_eval, onehot_eval, df_eval, yvar=\"shash_error\"):\n",
    "    # make predictions\n",
    "    mu_pred, sigma_pred, gamma_pred, tau_pred = prediction.params(x_eval, model)    \n",
    "\n",
    "    # convert y_pred to intensity corrections    \n",
    "    ann_correction  = prediction.percentile_value(mu_pred,sigma_pred,gamma_pred,tau_pred,percentile_frac=.5)\n",
    "    true_correction = df_eval['OBDV'].to_numpy()  \n",
    "\n",
    "    # convert to full intensity predictions\n",
    "    cons_intensity  = df_eval['VMXC'].to_numpy()\n",
    "    ann_intensity   = cons_intensity + ann_correction\n",
    "    true_intensity  = cons_intensity + true_correction\n",
    "\n",
    "    # compute errors\n",
    "    shash_error  = np.abs(ann_intensity - true_intensity)\n",
    "    cons_error = np.abs(cons_intensity - true_intensity)\n",
    "\n",
    "    # covert shash prediction bounds\n",
    "    shash_low_correction  = prediction.percentile_value(mu_pred,sigma_pred,gamma_pred,tau_pred,percentile_frac=.25)\n",
    "    shash_high_correction = prediction.percentile_value(mu_pred,sigma_pred,gamma_pred,tau_pred,percentile_frac=.75)\n",
    "    iqr                   = shash_high_correction - shash_low_correction\n",
    "\n",
    "    print(stats.spearmanr(iqr,shash_error))\n",
    "    print(stats.pearsonr(iqr,shash_error))\n",
    "\n",
    "    df_eval['iqr'] = iqr\n",
    "    df_eval['shash_error'] = shash_error\n",
    "    df_eval['cons_error'] = cons_error\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "    ROUND_BASE = 10\n",
    "\n",
    "    df_plot = df_eval.copy()\n",
    "    df_plot['iqr'] = pd.Series(df_plot['iqr']).apply(lambda x: custom_round(x, base=ROUND_BASE))\n",
    "    df_plot['iqr'] = pd.Series(df_plot['iqr']).apply(lambda x: ceiling_round(x, ceiling=30))\n",
    "    #-----------------------------------------------------------------------------\n",
    "    clr = colors[0]\n",
    "    if(yvar==\"cons_error\"):\n",
    "        clr =colors[1]\n",
    "    \n",
    "    for xvar in ('iqr',):#('sigma', 'range_66'):\n",
    "        g = sns.boxplot(\n",
    "            ax=ax,\n",
    "            data=df_plot,\n",
    "            x=xvar, y=yvar,\n",
    "            whis=None,\n",
    "            fliersize=0,\n",
    "            boxprops={'alpha':.2,\n",
    "                      'edgecolor': 'gray',\n",
    "                      'color': 'gray',\n",
    "                     },\n",
    "        )      \n",
    "        plt.setp(g.artists, edgecolor = '.4', facecolor='.95')\n",
    "        plt.setp(g.lines, color='.4')\n",
    "        ax = sns.stripplot(ax=ax,x=xvar, y=yvar, data=df_plot, color=clr, size=3.,alpha=.75,)\n",
    "        # plt.axhline(y=0,color='k', alpha=.25, linewidth=1)\n",
    "\n",
    "        # g.set_xlim(-.5,6.5)\n",
    "\n",
    "        ax.set_xlabel('ANN Predicted IQR [knots]')\n",
    "        if(yvar==\"cons_error\"):\n",
    "            ax.set_title('(c) Consensus Error vs ANN Predicted Uncertainty', fontsize=FS, color='k')    \n",
    "            ax.set_ylabel('Consensus Absolute Error [knots]')\n",
    "        else:\n",
    "            ax.set_title('(b) ANN Error vs ANN Predicted Uncertainty', fontsize=FS, color='k')            \n",
    "            ax.set_ylabel('ANN Absolute Error [knots]')        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Network functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model_name = (\n",
    "        exp_name + \"_\" + \n",
    "        str(TESTING_YEARS) + '_' +\n",
    "        settings[\"uncertainty_type\"] + '_' + \n",
    "        f\"network_seed_{network_seed}_rng_seed_{settings['rng_seed']}\"\n",
    "    )\n",
    "    pprint.pprint(model_name)\n",
    "    \n",
    "    if settings[\"uncertainty_type\"] == \"bnn\":       \n",
    "        model = build_model.build_bnn_model(\n",
    "            x_train,\n",
    "            onehot_train,\n",
    "            hiddens=settings[\"hiddens\"],\n",
    "            output_shape=onehot_train.shape[1],\n",
    "            act_fun=settings[\"act_fun\"],\n",
    "        )\n",
    "        \n",
    "    elif settings[\"uncertainty_type\"] == \"mcdrop\":       \n",
    "        model = build_model.build_mcdrop_model(\n",
    "            x_train,\n",
    "            onehot_train,\n",
    "            dropout_rate=settings[\"dropout_rate\"],                \n",
    "            hiddens=settings[\"hiddens\"],\n",
    "            output_shape=onehot_train.shape[1],\n",
    "            act_fun=settings[\"act_fun\"],\n",
    "        )\n",
    "\n",
    "    elif settings[\"uncertainty_type\"][:5] == \"shash\": \n",
    "        model = build_model.build_shash_model(\n",
    "            x_train,\n",
    "            onehot_train,\n",
    "            hiddens=settings[\"hiddens\"],\n",
    "            output_shape=onehot_train.shape[1],\n",
    "            ridge_penalty=settings[\"ridge_param\"],\n",
    "            act_fun=settings[\"act_fun\"],\n",
    "        )\n",
    "        \n",
    "    model.load_weights(MODEL_PATH + model_name + \"_weights.h5\")\n",
    "        \n",
    "    return model, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, x_eval):\n",
    "    runs = 5_000\n",
    "    shash_cpd = np.zeros((np.shape(x_eval)[0],len(SHASH_INCS)))\n",
    "    shash_med = np.zeros((np.shape(x_eval)[0],))\n",
    "    mc_cpd = np.zeros((np.shape(x_eval)[0],runs))\n",
    "    tf.random.set_seed(network_seed)\n",
    "\n",
    "    if settings[\"uncertainty_type\"] == \"bnn\":               \n",
    "        for i in tqdm(range(0,runs)):\n",
    "            mc_cpd[:,i] = np.reshape(model.predict(x_eval),np.shape(mc_cpd)[0])\n",
    "        return mc_cpd, np.median(mc_cpd,axis=1)\n",
    "    \n",
    "    elif settings[\"uncertainty_type\"] == \"mcdrop\":  \n",
    "        # loop through runs for mcdrop calculation  \n",
    "        for i in tqdm(range(0,runs)):\n",
    "            mc_cpd[:,i] = np.reshape(model(x_eval,training=True),np.shape(mc_cpd)[0])    \n",
    "        return mc_cpd, np.median(mc_cpd,axis=1)\n",
    "    \n",
    "    elif settings[\"uncertainty_type\"][:5] == \"shash\": \n",
    "        # loop through samples for shash calculation and get PDF for each sample\n",
    "        for j in tqdm(range(0,np.shape(shash_cpd)[0])):\n",
    "            mu_pred, sigma_pred, gamma_pred, tau_pred = prediction.params( x_eval[np.newaxis,j], model )\n",
    "            shash_cpd[j,:] = shash.prob(SHASH_INCS, mu_pred, sigma_pred, gamma_pred, tau_pred)    \n",
    "            shash_med[j]   = shash.median(mu_pred,sigma_pred,gamma_pred,tau_pred)\n",
    "        return shash_cpd, shash_med\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError('no such uncertainty type')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "for exp_name in EXP_NAME_LIST:\n",
    "    settings = experiment_settings.get_settings(exp_name)\n",
    "\n",
    "    TESTING_YEARS = TESTING_YEAR_VECTOR[exp_name]\n",
    "    settings[\"years_test\"] = (TESTING_YEARS,)\n",
    "    \n",
    "    RNG_SEED = PLOT_SEED_VECTOR[exp_name]\n",
    "    settings['rng_seed'] = RNG_SEED\n",
    "    seed_dict = {}\n",
    "    \n",
    "    NETWORK_SEED_LIST = [settings[\"rng_seed\"]]\n",
    "    network_seed = NETWORK_SEED_LIST[0]\n",
    "    tf.random.set_seed(network_seed)  # This sets the global random seed.  \n",
    "\n",
    "    # get the data\n",
    "    (\n",
    "        data_summary,        \n",
    "        x_train,\n",
    "        onehot_train,\n",
    "        x_val,\n",
    "        onehot_val,\n",
    "        x_test,\n",
    "        onehot_test,        \n",
    "        x_valtest,\n",
    "        onehot_valtest,\n",
    "        df_train,\n",
    "        df_val,\n",
    "        df_test,\n",
    "        df_valtest,\n",
    "    ) = build_hurricane_data(DATA_PATH, settings, verbose=0)\n",
    "    # x_eval, onehot_eval, df_eval = x_test, onehot_test, df_test\n",
    "\n",
    "    # load the correct model\n",
    "    model, model_name = load_model()\n",
    "\n",
    "    # get predictions\n",
    "    pred_test = get_predictions(model, x_test)\n",
    "    pred_valtest = get_predictions(model, x_valtest)\n",
    "\n",
    "    seed_dict[network_seed] = {'pred_cpd_test': pred_test[0], \n",
    "                               'pred_med_test': pred_test[1],\n",
    "                               'pred_cpd_valtest': pred_valtest[0], \n",
    "                               'pred_med_valtest': pred_valtest[1], \n",
    "                              }    \n",
    "    pred_dict[exp_name] = seed_dict\n",
    "\n",
    "print(pred_dict.keys())\n",
    "# pred_dict['paper1_EPCP96'][605]['pred_cpd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot single network results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_DATA = '_valtest'\n",
    "#------------------------------------------\n",
    "\n",
    "if PLOT_DATA == '_valtest':\n",
    "    x_eval = x_valtest\n",
    "    onehot_eval = onehot_valtest\n",
    "    df_eval = df_valtest\n",
    "    pred_eval = pred_dict[exp_name][RNG_SEED]['pred_cpd' + PLOT_DATA]    \n",
    "elif PLOT_DATA == '_test':\n",
    "    x_eval = x_test\n",
    "    onehot_eval = onehot_test\n",
    "    df_eval = df_test    \n",
    "    pred_eval = pred_dict[exp_name][RNG_SEED]['pred_cpd' + PLOT_DATA]    \n",
    "else:\n",
    "    raise NotImplementedError('no such PLOT_DATA')\n",
    "\n",
    "imp.reload(analysis_plots)\n",
    "f, axs = plt.subplots(1, 3, figsize=(15/2*3,5))\n",
    "\n",
    "# plot sample\n",
    "ax1 = axs[1]\n",
    "plot_iqr_vs_error(ax1,model, x_eval, onehot_eval, df_eval, yvar=\"shash_error\")\n",
    "ax1.set_ylim(0,65)\n",
    "ax1.set_yticks(np.arange(0,70,10),np.arange(0,70,10))\n",
    "ax1.set_xticks((0,1,2,3))\n",
    "ax1.set_xticklabels(['0-10','10-20','20-30','30+'])    \n",
    "format_spines(ax1)\n",
    "\n",
    "ax2 = axs[2]\n",
    "plot_iqr_vs_error(ax2,model, x_eval, onehot_eval, df_eval, yvar=\"cons_error\")\n",
    "ax2.set_ylim(0,65)\n",
    "ax2.set_yticks(np.arange(0,70,10),np.arange(0,70,10))\n",
    "ax2.set_xticks((0,1,2,3))\n",
    "ax2.set_xticklabels(['0-10','10-20','20-30','30+'])    \n",
    "format_spines(ax2)\n",
    "\n",
    "# plot PITS\n",
    "ax = axs[0]\n",
    "analysis_plots.plot_pits(ax, x_eval, onehot_eval, model, pred_eval, None)\n",
    "plt.ylim(0,.2)\n",
    "plt.title('(a) PIT histogram comparison', fontsize=FS, color='k')\n",
    "format_spines(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "ax.get_legend().remove()\n",
    "# plt.savefig(FIGURE_PATH + 'pit_error_iqr_' + model_name + '.png', dpi=dpiFig)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STORM_NAME = \"MICHAEL\"\n",
    "# STORM_NAME = \"MARIE\"\n",
    "# STORM_NAME = \"MARIA\"\n",
    "STORM_NAME = \"FLORENCE\"\n",
    "# STORM_NAME = \"HARVEY\"\n",
    "\n",
    "TESTING_YEAR = TESTING_YEAR_VECTOR[exp_name]\n",
    "LEADTIME = df_test[\"ftime(hr)\"].unique()[0]\n",
    "df_storm = df_test[df_test[\"Name\"]==STORM_NAME]\n",
    "df_storm = df_storm.sort_values(by=\"time\")\n",
    "istorm = df_storm.index\n",
    "display(df_storm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ri = pd.read_csv(PREDICTION_PATH + \"shash3_bestValTestingPredictions.csv\")\n",
    "storm_ri = df_ri[(df_ri[\"Name\"]==STORM_NAME) &\n",
    "                 (df_ri[\"year\"]==TESTING_YEAR) &\n",
    "                 (df_ri[\"ftime(hr)\"]==LEADTIME)\n",
    "                ]\n",
    "# storm_ri[\"Unnamed: 0\"]\n",
    "# storm_ri\n",
    "\n",
    "# a = df_ri[df_ri[\"shash_pr_ri\"]>.8]\n",
    "# # plt.hist(a[\"shash_pr_ri\"])\n",
    "# # plt.hist(a[\"clim_pr_ri\"])\n",
    "# print(np.mean(a[\"shash_pr_ri\"]), np.mean(a[\"clim_pr_ri\"]))\n",
    "\n",
    "# display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_intensity = df_test[\"VMXC\"] + df_test[\"OBDV\"]\n",
    "cons_intensity = df_test[\"VMXC\"]\n",
    "t0max = df_test[\"VMAX0\"]\n",
    "y_pred = pred_dict[exp_name][PLOT_SEED_VECTOR[exp_name]][\"pred_cpd_test\"]\n",
    "# SHASH_INCS+cons_intensity[sample],pred_dict[exp_name][RNG_SEED]['pred_cpd_test'][sample,:],\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 4\n",
    "nrows = int(np.ceil(len(istorm)/ncols))\n",
    "fig, axs = plt.subplots(nrows,ncols,figsize=(3.5*ncols,nrows*2.5))\n",
    "axs = axs.flatten()\n",
    "ymax = 1.05*np.max(y_pred[istorm,:][:])\n",
    "\n",
    "for iloop,index in enumerate(istorm):\n",
    "        \n",
    "    ax = axs[iloop]\n",
    "    ax.plot(SHASH_INCS+cons_intensity[index],\n",
    "            y_pred[index,:],\n",
    "            color=colors[0],\n",
    "            linewidth=3,\n",
    "            label='SHASH3',\n",
    "           )\n",
    "    ax.axvline(x=cons_intensity[index],linestyle='-',color=colors[1],linewidth=3,alpha=.75,label='Consensus')       \n",
    "    ax.axvline(x=true_intensity[index],linestyle='--',color='gray',linewidth=3,alpha=.75,label='BEST TRACK')    \n",
    "    ax.plot(t0max[index],\n",
    "            0.0,\n",
    "            marker='o',\n",
    "            markersize=9,\n",
    "            linestyle=None,\n",
    "            markerfacecolor='white',\n",
    "            color='gray',\n",
    "            label='T0MAX',\n",
    "           )\n",
    "    \n",
    "    # set axes things\n",
    "    ax.set_xlabel('knots')\n",
    "    ax.set_xticks(np.arange(0,200,25),np.arange(0,200,25))\n",
    "    ax.set_yticks(np.arange(0,.35,.05),np.arange(0,.35,.05).round(2)) \n",
    "    ax.set_ylim(-0.004,ymax)\n",
    "    ax.set_xlim(0,165)                            \n",
    "    details = get_storm_details(df_test,index)\n",
    "    ax.set_title(details[len(STORM_NAME)+1:details.find('@')])\n",
    "    format_spines(ax)\n",
    "    \n",
    "    \n",
    "    # check if RI event\n",
    "    try:\n",
    "        if true_intensity[index] - t0max[index] >= RI_THRESH_DICT[LEADTIME]:\n",
    "            ri_factor = 1.\n",
    "        else:\n",
    "            ri_factor = 0.01\n",
    "        axs[-1].bar(x=iloop,\n",
    "                    height=ri_factor,\n",
    "                    color=\"gray\",\n",
    "                    edgecolor='white',\n",
    "                    width=.8,\n",
    "                    alpha=.25,\n",
    "                   )        \n",
    "\n",
    "        # plot RI probabilities\n",
    "        axs[-1].plot(iloop,\n",
    "                     storm_ri[storm_ri[\"Unnamed: 0\"]==index][\"shash_pr_ri\"],\n",
    "                     'o',\n",
    "                     markersize=6,\n",
    "                     color=colors[0],\n",
    "                    )\n",
    "        axs[-1].plot(iloop,\n",
    "                     storm_ri[storm_ri[\"Unnamed: 0\"]==index][\"clim_pr_ri\"],\n",
    "                     'o',\n",
    "                     markersize=4,\n",
    "                     color=colors[1],\n",
    "                    )  \n",
    "    except:\n",
    "        pass\n",
    "for i in np.arange((iloop+1),(len(axs)-1)):\n",
    "    axs[i].axis('off') \n",
    "    \n",
    "# plot RI    \n",
    "format_spines(axs[-1])\n",
    "axs[-1].set_xticks(range(len(istorm)), \n",
    "                   df_test[\"time\"][istorm],\n",
    "                   rotation=45,\n",
    "                  )\n",
    "axs[-1].set_yticks((0,.5,1),(0,.5,1))\n",
    "axs[-1].set_ylim(-0.075,1)\n",
    "axs[-1].set_title('Pr[RI]')\n",
    "    \n",
    "\n",
    "axs[0].legend(fontsize=8) \n",
    "plt.suptitle(STORM_NAME + ' @' + str(LEADTIME) + 'hrs', fontsize=20)\n",
    "plt.tight_layout()    \n",
    "plt.savefig(FIGURE_PATH + 'caseStudy_' + model_name + '_' + STORM_NAME + '.png', dpi=dpiFig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
