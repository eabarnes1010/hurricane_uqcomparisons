{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing methods of hurricane forecast uncertainty\n",
    "##### author: Elizabeth A. Barnes, Randal J. Barnes and Mark DeMaria\n",
    "##### version: v0.1.0\n",
    "\n",
    "```\n",
    "conda create --name env-hurr-tfp python=3.9\n",
    "conda activate env-hurr-tfp\n",
    "pip install tensorflow==2.7.0\n",
    "pip install tensorflow-probability==0.15.0\n",
    "pip install --upgrade numpy scipy pandas statsmodels matplotlib seaborn \n",
    "pip install --upgrade palettable progressbar2 tabulate icecream flake8\n",
    "pip install --upgrade keras-tuner sklearn\n",
    "pip install --upgrade jupyterlab black isort jupyterlab_code_formatter\n",
    "pip install silence-tensorflow\n",
    "pip install tqdm\n",
    "```\n",
    "\n",
    "Use the command\n",
    "```python -m pip freeze > requirements.txt```\n",
    "to make a pip installation list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "import experiment_settings\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from build_data import build_hurricane_data\n",
    "from build_model import build_shash_model, build_bnn_model\n",
    "from custom_loss import compute_shash_NLL, compute_NLL\n",
    "from custom_metrics import CustomMAE, InterquartileCapture, SignTest\n",
    "from model_diagnostics import plot_history\n",
    "from save_model_run import save_model_run\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras import optimizers\n",
    "from training_instrumentation import TrainingInstrumentation\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "silence_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Randal J Barnes and Elizabeth A. Barnes\"\n",
    "__version__ = \"14 January 2022\"\n",
    "\n",
    "EXP_NAME = \"intensity0_AL72\"\n",
    "# EXP_NAME = \"intensity5_EPCP72\"\n",
    "# EXP_NAME = \"intensity102_EPCP48\"\n",
    "# EXP_NAME = \"intensity107_EPCP48\"\n",
    "# EXP_NAME = \"intensity201_EPCP48\"\n",
    "# EXP_NAME = \"intensity200_EPCP48\"\n",
    "\n",
    "DATA_PATH = \"data/\"\n",
    "MODEL_PATH = \"saved_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'act_fun': 'relu',\n",
      " 'basin': 'AL',\n",
      " 'batch_size': 64,\n",
      " 'filename': 'nnfit_vlist_intensity_and_track_extended.dat',\n",
      " 'hiddens': [15, 10],\n",
      " 'leadtime': 72,\n",
      " 'learning_rate': 0.0001,\n",
      " 'momentum': 0.9,\n",
      " 'n_epochs': 25000,\n",
      " 'n_train': 'max',\n",
      " 'n_val': 300,\n",
      " 'nesterov': True,\n",
      " 'patience': 300,\n",
      " 'ridge_param': 0.0,\n",
      " 'rng_seed': [888],\n",
      " 'target': 'intensity',\n",
      " 'uncertainty_type': 'shash3',\n",
      " 'undersample': False}\n"
     ]
    }
   ],
   "source": [
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"figure.dpi\"] = 150\n",
    "np.warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "settings = experiment_settings.get_settings(EXP_NAME)\n",
    "pprint.pprint(settings, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the intensity data tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "random_state must be an integer, array-like, a BitGenerator, a numpy RandomState, or None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ds/32cnt1m12110f2x86qdfzszh0001zl/T/ipykernel_14711/1700180881.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdf_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m ) = build_hurricane_data(DATA_PATH, settings, verbose=2)\n\u001b[0m",
      "\u001b[0;32m/Volumes/GoogleDrive/My Drive/WORK/RESEARCH/2022/hurricane_uqcomparisons/build_data.py\u001b[0m in \u001b[0;36mbuild_hurricane_data\u001b[0;34m(data_path, settings, verbose)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# Shuffle the rows in the df Dataframe, using the numpy rng.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# rng = np.random.default_rng(settings['rng_seed'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rng_seed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# Extract the x columns and the y column.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env-hurr-tfp/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   5282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5283\u001b[0m         \u001b[0;31m# Process random_state argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5284\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5286\u001b[0m         \u001b[0;31m# Check weights for compliance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env-hurr-tfp/lib/python3.9/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrandom_state\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    432\u001b[0m             \u001b[0;34m\"random_state must be an integer, array-like, a BitGenerator, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;34m\"a numpy RandomState, or None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: random_state must be an integer, array-like, a BitGenerator, a numpy RandomState, or None"
     ]
    }
   ],
   "source": [
    "(\n",
    "    x_train,\n",
    "    onehot_train,\n",
    "    x_val,\n",
    "    onehot_val,\n",
    "    x_eval,\n",
    "    onehot_eval,    \n",
    "    data_summary,\n",
    "    df_val,\n",
    "    df_eval,\n",
    ") = build_hurricane_data(DATA_PATH, settings, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystoping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=settings[\"patience\"],\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "training_callback = TrainingInstrumentation(\n",
    "    x_train,\n",
    "    onehot_train,\n",
    "    interval=10,\n",
    ")\n",
    "\n",
    "callbacks = [earlystoping_callback, training_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_SEED_LIST = [settings[\"rng_seed\"]]\n",
    "\n",
    "# NETWORK_SEED_LIST = [\n",
    "#       999, 18311, 59811, 96605, 57122, 71786, 67878, 33152, 22416, 81168, \n",
    "#     52157,   455, 98106, 37162, 58090, 90264, 25338, 27555, 92859, 13387,\n",
    "#     74723, 19736, 36842, 68050, 59711, 95199, 66418, 68997, 53431, 37786,\n",
    "#     79742, 74042,  8347, 49338, 96884, 14870, 88326,   921, 79436, 23564,\n",
    "#     2171,  89287, 31264, 22974, 31029, 97532,  4118, 20170, 77804, 67085,\n",
    "#     24752, 29814, 62255, 23602,  9709, 76607, 49259, 32678, 56290, 53251,\n",
    "#     66300, 40562, 10800, 94890, 14329, 94699, 43680, 71747, 64795, 68908,\n",
    "#     50914, 43933, 63123, 62693, 48125,  1701, 15552, 25529, 54440,  4657,\n",
    "#     9542,  77033, 96934, 49582, 34698, 82159, 20147, 83905, 17808, 21539,\n",
    "#     59896, 70793, 97321, 92170, 24675, 82172, 24226, 30460, 53389, 61562,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for network_seed in NETWORK_SEED_LIST:\n",
    "    tf.random.set_seed(network_seed)  # This sets the global random seed.\n",
    "\n",
    "    # Create the model name.\n",
    "    model_name = (\n",
    "        EXP_NAME + \"_\" + settings[\"uncertainty_type\"] + '_' + f\"network_seed_{network_seed}_rng_seed_{settings['rng_seed']}\"\n",
    "    )\n",
    "    pprint.pprint(model_name)\n",
    "\n",
    "    # Make, compile, and train the model\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    if settings[\"uncertainty_type\"] == \"bnn\":       \n",
    "        model = build_bnn_model(\n",
    "            x_train,\n",
    "            onehot_train,\n",
    "            hiddens=settings[\"hiddens\"],\n",
    "            output_shape=onehot_train.shape[1],\n",
    "            ridge_penalty=settings[\"ridge_param\"],\n",
    "            act_fun=settings[\"act_fun\"],\n",
    "        )        \n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(\n",
    "                learning_rate=settings[\"learning_rate\"],\n",
    "            ),\n",
    "            loss=compute_NLL,\n",
    "        )        \n",
    "        \n",
    "    elif settings[\"uncertainty_type\"][:5] == \"shash\":   \n",
    "        model = build_shash_model(\n",
    "            x_train,\n",
    "            onehot_train,\n",
    "            hiddens=settings[\"hiddens\"],\n",
    "            output_shape=onehot_train.shape[1],\n",
    "            ridge_penalty=settings[\"ridge_param\"],\n",
    "            act_fun=settings[\"act_fun\"],\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizers.SGD(\n",
    "                learning_rate=settings[\"learning_rate\"],\n",
    "                momentum=settings[\"momentum\"],\n",
    "                nesterov=settings[\"nesterov\"],\n",
    "            ),\n",
    "            loss=compute_shash_NLL,\n",
    "            metrics=[\n",
    "                CustomMAE(name=\"custom_mae\"),\n",
    "                InterquartileCapture(name=\"interquartile_capture\"),\n",
    "                SignTest(name=\"sign_test\"),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        \n",
    "    model.summary()\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        onehot_train,\n",
    "        validation_data=(x_val, onehot_val),\n",
    "        batch_size=settings[\"batch_size\"],\n",
    "        epochs=settings[\"n_epochs\"],\n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    stop_time = time.time()\n",
    "\n",
    "    # Display the results, and save the model rum.\n",
    "    best_epoch = np.argmin(history.history[\"val_loss\"])\n",
    "    fit_summary = {\n",
    "        \"network_seed\": network_seed,\n",
    "        \"elased_time\": stop_time - start_time,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"loss_train\": history.history[\"loss\"][best_epoch],\n",
    "        \"loss_valid\": history.history[\"val_loss\"][best_epoch],\n",
    "    }\n",
    "    pprint.pprint(fit_summary, width=80)\n",
    "    plot_history(history, model_name)\n",
    "\n",
    "    save_model_run(\n",
    "        data_summary,\n",
    "        fit_summary,\n",
    "        model,\n",
    "        MODEL_PATH,\n",
    "        model_name,\n",
    "        settings,\n",
    "        __version__,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + EXP_NAME + \" training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
